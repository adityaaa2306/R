cnn

----Random dataset -----
# Import necessary libraries
import tensorflow as tf
from tensorflow.keras import layers, models
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Set up image parameters
img_height = 150  # Set the image height
img_width = 150   # Set the image width
batch_size = 32   # Set the batch size

# Define dataset paths
train_dir = 'path_to_train_data'  # Path to the training dataset
test_dir = 'path_to_test_data'    # Path to the test dataset

# Data augmentation for training data
train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=40,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)

# For validation/test data, just rescale images
test_datagen = ImageDataGenerator(rescale=1./255)

# Load and preprocess the data from directories
train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=(img_height, img_width),
    batch_size=batch_size,
    class_mode='binary'  # Use 'categorical' for multi-class classification
)

test_generator = test_datagen.flow_from_directory(
    test_dir,
    target_size=(img_height, img_width),
    batch_size=batch_size,
    class_mode='binary'  # Use 'categorical' for multi-class classification
)

# Display some images from the training dataset
sample_images, sample_labels = next(train_generator)
plt.figure(figsize=(8, 8))
for i in range(9):
    plt.subplot(3, 3, i + 1)
    plt.imshow(sample_images[i], cmap='gray')
    plt.title(f"Label: {sample_labels[i]}")
    plt.axis('off')
plt.tight_layout()
plt.show()

# Build a simple CNN model
model = models.Sequential([
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(img_height, img_width, 3)),
    layers.MaxPooling2D(2, 2),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D(2, 2),
    layers.Conv2D(128, (3, 3), activation='relu'),
    layers.MaxPooling2D(2, 2),
    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.Dense(1, activation='sigmoid')  # Use 'softmax' for multi-class
])

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
history = model.fit(
    train_generator,
    epochs=10,
    validation_data=test_generator
)

# Evaluate the model
test_loss, test_acc = model.evaluate(test_generator, verbose=2)
print(f"Test accuracy: {test_acc:.4f}")

# Plot training & validation accuracy and loss
plt.figure(figsize=(12, 5))

# Accuracy plot
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.legend()
plt.title('Accuracy over Epochs')

# Loss plot
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.legend()
plt.title('Loss over Epochs')

plt.show()



---- OR ----- 

# Single Folder Image Dataset

# 1. Import libraries
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import matplotlib.pyplot as plt

# 2. Set image and training parameters
img_height = 150
img_width = 150
batch_size = 32
epochs = 10
data_dir = 'your_dataset'  # ğŸ” Path to your dataset with class subfolders

# 3. Create train & validation generators using validation_split
datagen = ImageDataGenerator(
    rescale=1./255,
    validation_split=0.2  # 80% for training, 20% for validation
)

train_generator = datagen.flow_from_directory(
    data_dir,
    target_size=(img_height, img_width),
    batch_size=batch_size,
    class_mode='categorical',  # Use 'binary' if 2 classes only
    subset='training',
    shuffle=True
)

val_generator = datagen.flow_from_directory(
    data_dir,
    target_size=(img_height, img_width),
    batch_size=batch_size,
    class_mode='categorical',
    subset='validation',
    shuffle=False
)

# 4. Visualize 9 random images
sample_images, sample_labels = next(train_generator)
plt.figure(figsize=(8, 8))
for i in range(9):
    plt.subplot(3, 3, i + 1)
    plt.imshow(sample_images[i])
    plt.title(f"Label: {train_generator.class_indices}")
    plt.axis('off')
plt.tight_layout()
plt.show()

# 5. Build the CNN model
model = models.Sequential([
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(img_height, img_width, 3)),
    layers.MaxPooling2D(2, 2),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D(2, 2),
    layers.Conv2D(128, (3, 3), activation='relu'),
    layers.MaxPooling2D(2, 2),
    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.Dense(train_generator.num_classes, activation='softmax')
])

# 6. Compile the model
model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# 7. Train the model
history = model.fit(
    train_generator,
    validation_data=val_generator,
    epochs=epochs
)

# 8. Evaluate model on validation data
val_loss, val_acc = model.evaluate(val_generator)
print(f"\nValidation Accuracy: {val_acc:.4f}")

# 9. Plot training history
plt.figure(figsize=(12, 5))

# Accuracy
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Val Accuracy')
plt.title('Accuracy over Epochs')
plt.legend()

# Loss
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.title('Loss over Epochs')
plt.legend()

plt.tight_layout()
plt.show()



--- mnist data ---- 

# Import necessary libraries
import tensorflow as tf
from tensorflow.keras import layers, models
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np

# Load the MNIST Dataset

(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()

# Display dataset shapes
print(f"Training data shape: {x_train.shape}, Training labels shape: {y_train.shape}")
print(f"Testing data shape: {x_test.shape}, Testing labels shape: {y_test.shape}")

#Visualize Sample Digits
# Display 9 sample images with labels
plt.figure(figsize=(8, 8))
for i in range(9):
    plt.subplot(3, 3, i + 1)
    plt.imshow(x_train[i], cmap='gray')
    plt.title(f"Label: {y_train[i]}")
    plt.axis('off')
plt.tight_layout()
plt.show()

#Show Class Distribution
# Plot the distribution of digits

plt.figure(figsize=(8, 5))
sns.countplot(x=y_train, palette="viridis")
plt.title("Class Distribution of MNIST Digits")
plt.xlabel("Digit")
plt.ylabel("Count")
plt.show()

# Preprocess the Data
# Normalize pixel values to the range [0, 1]

x_train = x_train.astype("float32") / 255.0
x_test = x_test.astype("float32") / 255.0

# Reshape for CNN input (batch_size, height, width, channels)
x_train = x_train.reshape(-1, 28, 28, 1)
x_test = x_test.reshape(-1, 28, 28, 1)

# One-hot encode the labels
y_train = tf.keras.utils.to_categorical(y_train, 10)
y_test = tf.keras.utils.to_categorical(y_test, 10)

# Build the CNN Model
model = models.Sequential([
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.Flatten(),
    layers.Dense(64, activation='relu'),
    layers.Dense(10, activation='softmax')  # 10 output classes for digits 0â€“9
])

# Compile the model with optimizer, loss function, and metric
model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Train the CNN
history = model.fit(
    x_train, y_train,
    epochs=10,
    batch_size=64,
    validation_data=(x_test, y_test)
)

# Evaluate performance on test data
test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)
print(f"\n Test Accuracy: {test_acc:.4f}")

# Plot training and validation accuracy/loss
plt.figure(figsize=(12, 5))

# Accuracy Plot
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title("Accuracy over Epochs")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend()

# Loss Plot
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title("Loss over Epochs")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()

plt.tight_layout()
plt.show()





rnn

---- NON TEXT DATA -----
 # Imports

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import SimpleRNN, Dense
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt

# Load Your Dataset

df = pd.read_csv("your_sequential_data.csv")  # Replace this

# Example: selecting features and target
features = df[['feature1', 'feature2', 'feature3']].values  # replace with actual column names
target = df['label'].values  # binary or regression target

#  Normalize Features

scaler = MinMaxScaler()
features_scaled = scaler.fit_transform(features)

#  Reshape Input for RNN: (samples, time_steps, features)
# For example: each sample is a sequence of 10 time steps
# You must reshape based on how your data is structured

sequence_length = 10

X = []
y = []

for i in range(len(features_scaled) - sequence_length):
    X.append(features_scaled[i:i+sequence_length])
    y.append(target[i+sequence_length])

X = np.array(X)
y = np.array(y)

# Train/Test Split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Build RNN Model

model = Sequential([
    SimpleRNN(64, input_shape=(X.shape[1], X.shape[2]), return_sequences=False),
    Dense(1, activation='sigmoid' if len(np.unique(y)) == 2 else 'linear')  # sigmoid for binary, linear for regression
])
model.summary()

# Compile Model
model.compile(
    optimizer='adam',
    loss='binary_crossentropy' if len(np.unique(y)) == 2 else 'mse',
    metrics=['accuracy'] if len(np.unique(y)) == 2 else ['mae']
)

# Train Model
history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)

# Evaluate
metrics = model.evaluate(X_test, y_test)
print(f"\nTest Results - Loss: {metrics[0]}, Metric: {metrics[1]}")

# Plot Accuracy or MAE

plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Loss')
plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend()

if 'accuracy' in history.history:
    plt.subplot(1, 2, 2)
    plt.plot(history.history['accuracy'], label='Train Accuracy')
    plt.plot(history.history['val_accuracy'], label='Val Accuracy')
    plt.title('Accuracy')
else:
    plt.subplot(1, 2, 2)
    plt.plot(history.history['mae'], label='Train MAE')
    plt.plot(history.history['val_mae'], label='Val MAE')
    plt.title('Mean Absolute Error')

plt.tight_layout()
plt.show()




---- TEXT DATA -----

# import 
import pandas as pd
import numpy as np
import re
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, SimpleRNN, Dense
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

# LOAD DATA 
df = pd.read_csv("your_data.csv")  # Change this to your CSV file
text_column = 'text_column_name'   # Column with text data
label_column = 'label_column_name' # Column with binary label (0 or 1)

# Clean the Text

def clean_text(text):
    text = str(text).lower()
    text = re.sub(r'[^a-z0-9\s]', '', text)
    return text

df['cleaned'] = df[text_column].apply(clean_text)

#  Tokenization & Padding
vocab_size = 5000
max_len = 100

tokenizer = Tokenizer(num_words=vocab_size, oov_token="<OOV>")
tokenizer.fit_on_texts(df['cleaned'])

sequences = tokenizer.texts_to_sequences(df['cleaned'])
padded = pad_sequences(sequences, maxlen=max_len, padding='post', truncating='post')

# Train-Test Split

X_train, X_test, y_train, y_test = train_test_split(
    padded, df[label_column].values, test_size=0.2, random_state=42
)

# RNN MODEL 
model = Sequential([
    Embedding(input_dim=vocab_size, output_dim=32, input_length=max_len),
    SimpleRNN(32),
    Dense(1, activation='sigmoid')
])

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the Model

history = model.fit(
    X_train, y_train, epochs=5, batch_size=32, validation_split=0.2
)

# Evaluation

loss, accuracy = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {accuracy:.4f}")

# Visualization

plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Accuracy'); plt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Loss'); plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend()

plt.tight_layout()
plt.show()

# Predict Samples

y_pred_prob = model.predict(X_test)
y_pred = (y_pred_prob > 0.5).astype("int32").flatten()

# Show Predictions for First 10 Samples
print("\nSample Predictions:")
for i in range(10):
    print(f"Text: {df[text_column].iloc[i][:60]}...")
    print(f"Actual: {y_test[i]}, Predicted: {y_pred[i]}")
   Â print('-'Â *Â 50)





img preproc

import cv2
import numpy as np
import matplotlib.pyplot as plt
import os
from PIL import Image, ImageEnhance

# Load Image
img = cv2.imread('C:/Users/konde/Desktop/flowers/lilies_00072.jpg')  # Update this path as per your folder structure
plt.subplot(1, 2, 1)
plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
plt.title("Original Image")
print("Original Image Shape:", img.shape)

# Resize to 200x200
img1 = cv2.resize(img, (200, 200))
plt.subplot(1, 2, 2)
plt.imshow(cv2.cvtColor(img1, cv2.COLOR_BGR2RGB))
plt.title("Resized Image (200x200)")
plt.show()

# Brightness and Contrast Adjustment
brightness = 6.5
contrast = 1.5
adjusted = cv2.addWeighted(img, contrast, np.zeros(img.shape, img.dtype), 0, brightness)

cv2.imwrite('images/modified_image.jpg', adjusted)

plt.subplot(1, 2, 1)
plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
plt.title("Original")

plt.subplot(1, 2, 2)
plt.imshow(cv2.cvtColor(adjusted, cv2.COLOR_BGR2RGB))
plt.title("Brightness & Contrast")
plt.show()

# Blur
def blur_img():
    img = cv2.imread('C:/Users/konde/Desktop/flowers/lilies_00072.jpg')
    img = cv2.resize(img, (400, 400))
    blurred = cv2.blur(img, (9, 9))

    plt.subplot(1, 2, 1)
    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
    plt.title("Original")

    plt.subplot(1, 2, 2)
    plt.imshow(cv2.cvtColor(blurred, cv2.COLOR_BGR2RGB))
    plt.title("Blurred")
    plt.show()

blur_img()

# Rotation and Flip
rotated = cv2.rotate(img, cv2.ROTATE_90_COUNTERCLOCKWISE)
flipped = cv2.flip(img, 0)

plt.subplot(1, 2, 1)
plt.imshow(cv2.cvtColor(rotated, cv2.COLOR_BGR2RGB))
plt.title("Rotated 90Â° CCW")

plt.subplot(1, 2, 2)
plt.imshow(cv2.cvtColor(flipped, cv2.COLOR_BGR2RGB))
plt.title("Flipped Vertically")
plt.show()

-- Augmentation -- 
from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img

# Define folder containing images
flowers_path = 'C:/Users/konde/Desktop/flowers'  # Update as per your folder
image_files_in_flowers = os.listdir(flowers_path)

# Display first 5 original images
plt.figure(figsize=(15, 5))
for i in range(5):
    img_path = os.path.join(flowers_path, image_files_in_flowers[i])
    img = cv2.imread(img_path)
    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    plt.subplot(1, 5, i+1)
    plt.imshow(img_rgb)
    plt.axis('off')
    plt.title(image_files_in_flowers[i], fontsize=10)
plt.tight_layout()
plt.show()

# Image Augmentation
datagen = ImageDataGenerator(
    rotation_range=40,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)

# Load a sample image and convert to array
sample_image_path = os.path.join(flowers_path, image_files_in_flowers[0])
img = load_img(sample_image_path)  # PIL Image
x = img_to_array(img)              # Numpy array
x = np.expand_dims(x, axis=0)      # (1, h, w, 3)

# Generate and show augmented images
plt.figure(figsize=(12, 12))
aug_iter = datagen.flow(x, batch_size=1)

for i in range(5):
    aug_img = next(aug_iter)[0].astype('uint8')
    plt.subplot(1, 5, i + 1)
    plt.imshow(aug_img)
    plt.axis('off')
    plt.title(f'Augmented {i+1}')

plt.tight_layout()
plt.show()





ann

# Universal ANN Template for Any CSV Dataset (Jupyter Notebook Version)

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler
from keras.models import Sequential
from keras.layers import Dense
from keras.optimizers import Adam
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix

# --- Load Dataset ---
csv_path = "your_dataset.csv"  # Change this to your CSV file
data = pd.read_csv(csv_path)
print("Dataset loaded successfully.")
print(data.head())

# --- Identify Target Column ---
target_column = 'target'  # <-- CHANGE THIS to the actual target column name in your dataset
X = data.drop(columns=[target_column])
y = data[target_column]

# --- Encode Categorical Variables ---
label_encoders = {}
for column in X.columns:
    if X[column].dtype == 'object' or X[column].dtype.name == 'category':
        le = LabelEncoder()
        X[column] = le.fit_transform(X[column].astype(str))
        label_encoders[column] = le

# Encode target if it's categorical
if y.dtype == 'object' or y.dtype.name == 'category':
    target_encoder = LabelEncoder()
    y = target_encoder.fit_transform(y)

# --- Feature Scaling ---
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)

# --- Train-Test Split ---
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# --- Build ANN Model ---
model = Sequential()
model.add(Dense(64, activation='relu', input_dim=X_train.shape[1]))
model.add(Dense(32, activation='relu'))
model.add(Dense(1, activation='sigmoid' if len(np.unique(y)) == 2 else 'softmax'))

# --- Compile the Model ---
model.compile(optimizer=Adam(), loss='binary_crossentropy' if len(np.unique(y)) == 2 else 'sparse_categorical_crossentropy',
              metrics=['accuracy'])

# --- Train the Model ---
history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=1)

# --- Evaluate the Model ---
loss, accuracy = model.evaluate(X_test, y_test)
print(f"\nTest Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}")

# --- Predictions & Reports ---
y_pred = (model.predict(X_test) > 0.5).astype("int32") if len(np.unique(y)) == 2 else np.argmax(model.predict(X_test), axis=1)

print("\nClassification Report:")
print(classification_report(y_test, y_pred))

print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred))

# --- Plot Training History ---
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)
plt.show()




-------------------------------------------

implemented assignment 
# Load dataset
df = pd.read_csv("online_course_engagement_data.csv")  # Replace with your dataset

from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder
from sklearn.compose import ColumnTransformer

# Identify categorical columns
categorical_columns = df.select_dtypes(include=['object']).columns.tolist()

# Apply Label Encoding or One-Hot Encoding
if categorical_columns:
    # One-Hot Encode categorical variables
    ct = ColumnTransformer([("encoder", OneHotEncoder(), categorical_columns)], remainder='passthrough')
    X = ct.fit_transform(df.iloc[:, :-1])  # Transform features

    # Convert to float (if necessary)
    X = np.array(X, dtype=np.float32)
else:
    X = df.iloc[:, :-1].values  # No categorical features

# Encode target variable if categorical
if df.iloc[:, -1].dtype == 'object':
    label_encoder = LabelEncoder()
    y = label_encoder.fit_transform(df.iloc[:, -1])  # Convert target to numeric
else:
    y = df.iloc[:, -1].values  # Already numeric

# Split dataset into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Build Neural Network Model
model = Sequential()
model.add(Dense(16, input_dim=X_train.shape[1], activation='relu'))
model.add(Dense(8, activation='relu'))
model.add(Dense(1, activation='sigmoid'))  # Change to softmax if multi-class

# Compile the model
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
history = model.fit(X_train, y_train, epochs=100, batch_size=10, validation_data=(X_test, y_test), verbose=1)

# Predictions
y_pred_nn = (model.predict(X_test) > 0.5).astype(int)

# Evaluate Neural Network
nn_accuracy = accuracy_score(y_test, y_pred_nn)
print(f"Neural Network Accuracy: {nn_accuracy:.4f}")

# Plot Training Loss and Accuracy
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Training & Validation Loss')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Training & Validation Accuracy')
plt.legend()

plt.show()





activ func

# Import Required Libraries

import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import math 

# Enable inline plotting in Jupyter
%matplotlib inline

# Binary Step Activation Function
def binarystep(x):
    return np.heaviside(x, 1)

x = np.linspace(-10, 10, 500)
plt.plot(x, binarystep(x), label='Binary Step')
plt.xlabel("x")
plt.ylabel("f(x)")
plt.title("Binary Step Function")
plt.grid(True)
plt.show()

# Sigmoid Activation Function
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

plt.plot(x, sigmoid(x), label='Sigmoid', color='green')
plt.xlabel("x")
plt.ylabel("f(x)")
plt.title("Sigmoid Function")
plt.grid(True)
plt.show()

# Tanh Activation Function
def tanh(x):
    return np.tanh(x)

plt.plot(x, tanh(x), label='Tanh', color='purple')
plt.xlabel("x")
plt.ylabel("f(x)")
plt.title("Tanh Function")
plt.grid(True)
plt.show()

# ReLU Activation Function
def relu(x):
    return np.maximum(0, x)

plt.plot(x, relu(x), label='ReLU', color='orange')
plt.xlabel("x")
plt.ylabel("f(x)")
plt.title("ReLU Function")
plt.grid(True)
plt.show()

# Leaky ReLU Activation Function
def leaky_relu(x, alpha=0.01):
    return np.where(x > 0, x, alpha * x)

plt.plot(x, leaky_relu(x), label='Leaky ReLU', color='red')
plt.xlabel("x")
plt.ylabel("f(x)")
plt.title("Leaky ReLU Function")
plt.grid(True)
plt.show()






andorgate

AND/OR OPERATIONS

2-INPUT 

# Imports
import numpy as np
import matplotlib.pyplot as plt

# Sigmoid Activation Function
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

# AND Operation with One Hidden Layer
# One hidden layer parameter initialization
def initializeParameters(inputFeatures, neuronsInHiddenLayers, outputFeatures):
    W1 = np.random.randn(neuronsInHiddenLayers, inputFeatures)
    b1 = np.zeros((neuronsInHiddenLayers, 1))
    W2 = np.random.randn(outputFeatures, neuronsInHiddenLayers)
    b2 = np.zeros((outputFeatures, 1))
    return {"W1": W1, "b1": b1, "W2": W2, "b2": b2}

# Forward Propagation for one hidden layer
def forwardPropagation(X, Y, parameters):
    m = X.shape[1]
    W1, b1, W2, b2 = parameters["W1"], parameters["b1"], parameters["W2"], parameters["b2"]
    Z1 = np.dot(W1, X) + b1
    A1 = sigmoid(Z1)
    Z2 = np.dot(W2, A1) + b2
    A2 = sigmoid(Z2)
    cost = -np.sum(np.multiply(np.log(A2), Y) + np.multiply(np.log(1 - A2), 1 - Y)) / m
    cache = (Z1, A1, W1, b1, Z2, A2, W2, b2)
    return cost, cache, A2

# Backward Propagation
def backwardPropagation(X, Y, cache):
    m = X.shape[1]
    Z1, A1, W1, b1, Z2, A2, W2, b2 = cache
    dZ2 = A2 - Y
    dW2 = np.dot(dZ2, A1.T) / m
    db2 = np.sum(dZ2, axis=1, keepdims=True) / m
    dA1 = np.dot(W2.T, dZ2)
    dZ1 = dA1 * A1 * (1 - A1)
    dW1 = np.dot(dZ1, X.T) / m
    db1 = np.sum(dZ1, axis=1, keepdims=True) / m
    return {"dW1": dW1, "db1": db1, "dW2": dW2, "db2": db2}

# Parameter update
def updateParameters(parameters, gradients, learningRate):
    for key in parameters:
        parameters[key] -= learningRate * gradients["d" + key]
    return parameters

# Train AND operation with one hidden layer
X = np.array([[0, 0, 1, 1], [0, 1, 0, 1]])
Y = np.array([[0, 0, 0, 1]])
inputFeatures = X.shape[0]
neuronsInHiddenLayers = 2
outputFeatures = Y.shape[0]
epoch = 100000
learningRate = 0.01

parameters = initializeParameters(inputFeatures, neuronsInHiddenLayers, outputFeatures)
losses = np.zeros((epoch, 1))

for i in range(epoch):
    losses[i, 0], cache, A2 = forwardPropagation(X, Y, parameters)
    gradients = backwardPropagation(X, Y, cache)
    parameters = updateParameters(parameters, gradients, learningRate)

# Plotting the training loss
plt.figure()
plt.plot(losses)
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.title("AND Operation with One Hidden Layer")
plt.grid()
plt.show()

# Test prediction
_, _, A2 = forwardPropagation(X, Y, parameters)
prediction = (A2 > 0.5) * 1.0
print("Prediction (AND - One Hidden Layer):\n", prediction)


# II. AND Operation with Two Hidden Layers
# Two hidden layers initialization
def initializeParametersTwoHiddenLayers(inputFeatures, hiddenLayer1, hiddenLayer2, outputFeatures):
    W1 = np.random.randn(hiddenLayer1, inputFeatures)
    b1 = np.zeros((hiddenLayer1, 1))
    W2 = np.random.randn(hiddenLayer2, hiddenLayer1)
    b2 = np.zeros((hiddenLayer2, 1))
    W3 = np.random.randn(outputFeatures, hiddenLayer2)
    b3 = np.zeros((outputFeatures, 1))
    return {"W1": W1, "b1": b1, "W2": W2, "b2": b2, "W3": W3, "b3": b3}

# Forward Propagation for two hidden layers
def forwardPropagationTwoHiddenLayers(X, Y, parameters):
    m = X.shape[1]
    W1, b1, W2, b2, W3, b3 = parameters.values()
    Z1 = np.dot(W1, X) + b1
    A1 = sigmoid(Z1)
    Z2 = np.dot(W2, A1) + b2
    A2 = sigmoid(Z2)
    Z3 = np.dot(W3, A2) + b3
    A3 = sigmoid(Z3)
    cost = -np.sum(np.multiply(np.log(A3), Y) + np.multiply(np.log(1 - A3), 1 - Y)) / m
    cache = (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3)
    return cost, cache, A3

# Backward Propagation for two hidden layers
def backwardPropagationTwoHiddenLayers(X, Y, cache):
    m = X.shape[1]
    Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3 = cache
    dZ3 = A3 - Y
    dW3 = np.dot(dZ3, A2.T) / m
    db3 = np.sum(dZ3, axis=1, keepdims=True) / m
    dA2 = np.dot(W3.T, dZ3)
    dZ2 = dA2 * A2 * (1 - A2)
    dW2 = np.dot(dZ2, A1.T) / m
    db2 = np.sum(dZ2, axis=1, keepdims=True) / m
    dA1 = np.dot(W2.T, dZ2)
    dZ1 = dA1 * A1 * (1 - A1)
    dW1 = np.dot(dZ1, X.T) / m
    db1 = np.sum(dZ1, axis=1, keepdims=True) / m
    return {"dW1": dW1, "db1": db1, "dW2": dW2, "db2": db2, "dW3": dW3, "db3": db3}

# Update parameters
def updateParametersTwoHiddenLayers(parameters, gradients, learningRate):
    for key in parameters:
        parameters[key] -= learningRate * gradients["d" + key]
    return parameters

# Train AND operation with two hidden layers
hiddenLayer1 = 2
hiddenLayer2 = 2
parameters = initializeParametersTwoHiddenLayers(inputFeatures, hiddenLayer1, hiddenLayer2, outputFeatures)
losses = np.zeros((epoch, 1))

for i in range(epoch):
    losses[i, 0], cache, A3 = forwardPropagationTwoHiddenLayers(X, Y, parameters)
    gradients = backwardPropagationTwoHiddenLayers(X, Y, cache)
    parameters = updateParametersTwoHiddenLayers(parameters, gradients, learningRate)

plt.figure()
plt.plot(losses)
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.title("AND Operation with Two Hidden Layers")
plt.grid()
plt.show()

# Prediction
_, _, A3 = forwardPropagationTwoHiddenLayers(X, Y, parameters)
prediction = (A3 > 0.5) * 1.0
print("Prediction (AND - Two Hidden Layers):\n", prediction)


# III. OR Operation with One Hidden Layer
# OR gate dataset
X_or = np.array([[0, 0, 1, 1], [0, 1, 0, 1]])
Y_or = np.array([[0, 1, 1, 1]])

# Re-initialize parameters
parameters = initializeParameters(inputFeatures, neuronsInHiddenLayers, outputFeatures)
losses = np.zeros((epoch, 1))

# Train
for i in range(epoch):
    losses[i, 0], cache, A2 = forwardPropagation(X_or, Y_or, parameters)
    gradients = backwardPropagation(X_or, Y_or, cache)
    parameters = updateParameters(parameters, gradients, learningRate)

plt.figure()
plt.plot(losses)
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.title("OR Operation with One Hidden Layer")
plt.grid()
plt.show()

# Evaluate
_, _, A2 = forwardPropagation(X_or, Y_or, parameters)
prediction_or = (A2 > 0.5) * 1.0
print("Prediction (OR - One Hidden Layer):\n",Â prediction_or)




3-INPUT AND
import numpy as np
from matplotlib import pyplot as plt

# Sigmoid Function
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

# Initialization of the neural network parameters
# Initialized all the weights in the range of between 0 and 1
# Bias values are initialized to 0
def initializeParameters(inputFeatures, neuronsInHiddenLayers, outputFeatures):
    W1 = np.random.randn(neuronsInHiddenLayers, inputFeatures)    # 2 X 3 
    W2 = np.random.randn(outputFeatures, neuronsInHiddenLayers)   # 1 X 2  
    b1 = np.zeros((neuronsInHiddenLayers, 1))                     # 2 X 1 
    b2 = np.zeros((outputFeatures, 1))                            # 1 X 1 
    parameters = {"W1": W1, "b1": b1,
                  "W2": W2, "b2": b2}
    return parameters

# Forward Propagation
def forwardPropagation(X, Y, parameters):
    m = X.shape[1]
    W1 = parameters["W1"]
    W2 = parameters["W2"]
    b1 = parameters["b1"]
    b2 = parameters["b2"]
    Z1 = np.dot(W1, X) + b1
    A1 = sigmoid(Z1)
    Z2 = np.dot(W2, A1) + b2
    A2 = sigmoid(Z2)
    cache = (Z1, A1, W1, b1, Z2, A2, W2, b2)
    logprobs = np.multiply(np.log(A2), Y) + np.multiply(np.log(1 - A2), (1 - Y))
    cost = -np.sum(logprobs) / m
    return cost, cache, A2


# Backward Propagation
def backwardPropagation(X, Y, cache):
    m = X.shape[1]
    (Z1, A1, W1, b1, Z2, A2, W2, b2) = cache
    dZ2 = A2 - Y
    dW2 = np.dot(dZ2, A1.T) / m
    db2 = np.sum(dZ2, axis=1, keepdims=True)
    dA1 = np.dot(W2.T, dZ2)
    dZ1 = np.multiply(dA1, A1 * (1 - A1))
    dW1 = np.dot(dZ1, X.T) / m
    db1 = np.sum(dZ1, axis=1, keepdims=True) / m
    gradients = {"dZ2": dZ2, "dW2": dW2, "db2": db2,
                 "dZ1": dZ1, "dW1": dW1, "db1": db1}
    return gradients


# Updating the weights based on the negative gradients
def updateParameters(parameters, gradients, learningRate):
    parameters["W1"] = parameters["W1"] - learningRate * gradients["dW1"]
    parameters["W2"] = parameters["W2"] - learningRate * gradients["dW2"]
    parameters["b1"] = parameters["b1"] - learningRate * gradients["db1"]
    parameters["b2"] = parameters["b2"] - learningRate * gradients["db2"]
    return parameters

# Model to learn the AND truth table
X = np.array([[0, 0, 0, 0, 1, 1, 1, 1],
              [0, 0, 1, 1, 0, 0, 1, 1],
              [0, 1, 0, 1, 0, 1, 0, 1]])  # AND input
print(X)
Y = np.array([[0, 0, 0, 0, 0, 0, 0, 1]])  # AND output

# Define model parameters
neuronsInHiddenLayers = 2               # number of hidden layer neurons (2)
inputFeatures = X.shape[0]              # number of input features (3)
outputFeatures = Y.shape[0]             # number of output features (1)
parameters = initializeParameters(inputFeatures, neuronsInHiddenLayers, outputFeatures)
epoch = 100000
learningRate = 0.01
losses = np.zeros((epoch, 1))
for i in range(epoch):
    losses[i, 0], cache, A2 = forwardPropagation(X, Y, parameters)
    gradients = backwardPropagation(X, Y, cache)
    parameters = updateParameters(parameters, gradients, learningRate)

# Evaluating the performance
plt.figure()
plt.plot(losses)
plt.xlabel("EPOCHS")
plt.ylabel("Loss value")
plt.title("Training Loss Over Time")
plt.show()


# Testing
X_test = np.array([[0, 0, 0, 0, 1, 1, 1, 1],
                   [0, 0, 1, 1, 0, 0, 1, 1],
                   [0, 1, 0, 1, 0, 1, 0, 1]])  # AND input
cost, _, A2_test = forwardPropagation(X_test, Y, parameters)
prediction = (A2_test > 0.5) * 1.0

print(A2_test)
print(prediction)


3-INPUT OR 
import numpy as np
from matplotlib import pyplot as plt

# Sigmoid Function
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

# Initialization of the neural network parameters
def initializeParameters(inputFeatures, neuronsInHiddenLayers, outputFeatures):
    W1 = np.random.randn(neuronsInHiddenLayers, inputFeatures)
    W2 = np.random.randn(outputFeatures, neuronsInHiddenLayers)
    b1 = np.zeros((neuronsInHiddenLayers, 1))
    b2 = np.zeros((outputFeatures, 1))
    parameters = {"W1": W1, "b1": b1,
                  "W2": W2, "b2": b2}
    return parameters

# Forward Propagation
def forwardPropagation(X, Y, parameters):
    m = X.shape[1]
    W1 = parameters["W1"]
    W2 = parameters["W2"]
    b1 = parameters["b1"]
    b2 = parameters["b2"]
    Z1 = np.dot(W1, X) + b1
    A1 = sigmoid(Z1)
    Z2 = np.dot(W2, A1) + b2
    A2 = sigmoid(Z2)
    cache = (Z1, A1, W1, b1, Z2, A2, W2, b2)
    logprobs = np.multiply(np.log(A2), Y) + np.multiply(np.log(1 - A2), (1 - Y))
    cost = -np.sum(logprobs) / m
    return cost, cache, A2

# Backward Propagation
def backwardPropagation(X, Y, cache):
    m = X.shape[1]
    (Z1, A1, W1, b1, Z2, A2, W2, b2) = cache
    dZ2 = A2 - Y
    dW2 = np.dot(dZ2, A1.T) / m
    db2 = np.sum(dZ2, axis=1, keepdims=True) / m
    dA1 = np.dot(W2.T, dZ2)
    dZ1 = np.multiply(dA1, A1 * (1 - A1))
    dW1 = np.dot(dZ1, X.T) / m
    db1 = np.sum(dZ1, axis=1, keepdims=True) / m
    gradients = {"dZ2": dZ2, "dW2": dW2, "db2": db2,
                 "dZ1": dZ1, "dW1": dW1, "db1": db1}
    return gradients

# Updating the weights
def updateParameters(parameters, gradients, learningRate):
    parameters["W1"] -= learningRate * gradients["dW1"]
    parameters["W2"] -= learningRate * gradients["dW2"]
    parameters["b1"] -= learningRate * gradients["db1"]
    parameters["b2"] -= learningRate * gradients["db2"]
    return parameters

# OR truth table
X = np.array([[0, 0, 0, 0, 1, 1, 1, 1],
              [0, 0, 1, 1, 0, 0, 1, 1],
              [0, 1, 0, 1, 0, 1, 0, 1]])  # OR input
Y = np.array([[0, 1, 1, 1, 1, 1, 1, 1]])  # OR output

# Define model parameters
neuronsInHiddenLayers = 2
inputFeatures = X.shape[0]
outputFeatures = Y.shape[0]
parameters = initializeParameters(inputFeatures, neuronsInHiddenLayers, outputFeatures)

# Training settings
epoch = 100000
learningRate = 0.01
losses = np.zeros((epoch, 1))

for i in range(epoch):
    losses[i, 0], cache, A2 = forwardPropagation(X, Y, parameters)
    gradients = backwardPropagation(X, Y, cache)
    parameters = updateParameters(parameters, gradients, learningRate)

# Plot the loss curve
plt.figure()
plt.plot(losses)
plt.xlabel("EPOCHS")
plt.ylabel("Loss value")
plt.title("Training Loss Over Time")
plt.show()

# Testing
X_test = np.array([[0, 0, 0, 0, 1, 1, 1, 1],
                   [0, 0, 1, 1, 0, 0, 1, 1],
                   [0, 1, 0, 1, 0, 1, 0, 1]])  # OR input
cost, _, A2_test = forwardPropagation(X_test, Y, parameters)
prediction = (A2_test > 0.5) * 1.0

print(A2_test)
print(prediction)

3-INPUT AND + 2 HIDDEN LAYER + OUTPUT LAYER
import numpy as np
from matplotlib import pyplot as plt

# Sigmoid Activation Function
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

# Initialization of the neural network parameters with two hidden layers
def initializeParameters(inputFeatures, neuronsHidden1, neuronsHidden2, outputFeatures):
    # Weight and bias initialization for first hidden layer
    W1 = np.random.randn(neuronsHidden1, inputFeatures)    # Hidden Layer 1 Weights
    b1 = np.zeros((neuronsHidden1, 1))                      # Hidden Layer 1 Bias
    
    # Weight and bias initialization for second hidden layer
    W2 = np.random.randn(neuronsHidden2, neuronsHidden1)   # Hidden Layer 2 Weights
    b2 = np.zeros((neuronsHidden2, 1))                      # Hidden Layer 2 Bias
    
    # Weight and bias initialization for output layer
    W3 = np.random.randn(outputFeatures, neuronsHidden2)   # Output Layer Weights
    b3 = np.zeros((outputFeatures, 1))                      # Output Layer Bias
    
    parameters = {"W1": W1, "b1": b1, "W2": W2, "b2": b2, "W3": W3, "b3": b3}
    return parameters

# Forward Propagation with two hidden layers
def forwardPropagation(X, Y, parameters):
    m = X.shape[1]
    
    W1, b1 = parameters["W1"], parameters["b1"]
    W2, b2 = parameters["W2"], parameters["b2"]
    W3, b3 = parameters["W3"], parameters["b3"]
    
    # Layer 1
    Z1 = np.dot(W1, X) + b1
    A1 = sigmoid(Z1)
    
    # Layer 2
    Z2 = np.dot(W2, A1) + b2
    A2 = sigmoid(Z2)
    
    # Output Layer
    Z3 = np.dot(W3, A2) + b3
    A3 = sigmoid(Z3)
    
    cache = (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3)
    
    # Compute Cost
    logprobs = np.multiply(np.log(A3), Y) + np.multiply(np.log(1 - A3), (1 - Y))
    cost = -np.sum(logprobs) / m
    
    return cost, cache, A3

# Backward Propagation with two hidden layers
def backwardPropagation(X, Y, cache):
    m = X.shape[1]
    
    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache
    
    dZ3 = A3 - Y
    dW3 = np.dot(dZ3, A2.T) / m
    db3 = np.sum(dZ3, axis=1, keepdims=True) / m
    
    dA2 = np.dot(W3.T, dZ3)
    dZ2 = np.multiply(dA2, A2 * (1 - A2))
    dW2 = np.dot(dZ2, A1.T) / m
    db2 = np.sum(dZ2, axis=1, keepdims=True) / m
    
    dA1 = np.dot(W2.T, dZ2)
    dZ1 = np.multiply(dA1, A1 * (1 - A1))
    dW1 = np.dot(dZ1, X.T) / m
    db1 = np.sum(dZ1, axis=1, keepdims=True) / m
    
    gradients = {"dW1": dW1, "db1": db1, "dW2": dW2, "db2": db2, "dW3": dW3, "db3": db3}
    return gradients

# Updating the weights based on the gradients
def updateParameters(parameters, gradients, learningRate):
    parameters["W1"] -= learningRate * gradients["dW1"]
    parameters["b1"] -= learningRate * gradients["db1"]
    parameters["W2"] -= learningRate * gradients["dW2"]
    parameters["b2"] -= learningRate * gradients["db2"]
    parameters["W3"] -= learningRate * gradients["dW3"]
    parameters["b3"] -= learningRate * gradients["db3"]
    return parameters

# Defining the AND dataset
X = np.array([[0, 0, 0, 0, 1, 1, 1, 1],
              [0, 0, 1, 1, 0, 0, 1, 1],
              [0, 1, 0, 1, 0, 1, 0, 1]])  # 3-input AND gate

Y = np.array([[0, 0, 0, 0, 0, 0, 0, 1]])  # Expected AND outputs

# Define model parameters
neuronsHidden1 = 4  # First hidden layer neurons
neuronsHidden2 = 3  # Second hidden layer neurons
inputFeatures = X.shape[0]  # Number of input features
outputFeatures = Y.shape[0]  # Output layer neurons
parameters = initializeParameters(inputFeatures, neuronsHidden1, neuronsHidden2, outputFeatures)

# Training settings
epoch = 100000
learningRate = 0.01
losses = np.zeros((epoch, 1))

# Training Loop
for i in range(epoch):
    losses[i, 0], cache, A3 = forwardPropagation(X, Y, parameters)
    gradients = backwardPropagation(X, Y, cache)
    parameters = updateParameters(parameters, gradients, learningRate)

# Plot training loss
plt.figure()
plt.plot(losses)
plt.xlabel("EPOCHS")
plt.ylabel("Loss value")
plt.title("Training Loss Over Time")
plt.show()

# Testing the trained model
X_test = np.array([[0, 0, 0, 0, 1, 1, 1, 1],
                   [0, 0, 1, 1, 0, 0, 1, 1],
                   [0, 1, 0, 1, 0, 1, 0, 1]])

cost, _, A3_test = forwardPropagation(X_test, Y, parameters)
prediction = (A3_test > 0.5) * 1.0  # Convert probabilities to binary output

print("Predicted Output Probabilities:")
print(A3_test)
print("Final Binary Predictions:")
print(prediction)


KERAS NEURAL NETWORK - 
from tensorflow.keras.models import Sequential  
from tensorflow.keras.layers import Dense  
import numpy as np
#Defining the Training Data and Target Output
training_data = np.array([[0,0], [0,1], [1,0], [1,1]], "float32")  
target_data = np.array([[0], [1], [1], [0]], "float32") 
#Creating the neural network model
model = Sequential()  
model.add(Dense(16, input_dim=2, activation='relu'))  
model.add(Dense(1, activation='sigmoid'))
#Compiling the model 
model.compile(loss='mean_squared_error', optimizer='adam', metrics=['binary_accuracy'])  
#Compiling the model 
model.compile(loss='mean_squared_error', optimizer='adam', metrics=['binary_accuracy'])  
#training the model
model.fit(training_data, target_data, epochs=1000)  
# evaluation the model
scores = model.evaluate(training_data, target_data) 
# Train the model and store history
history = model.fit(training_data, target_data, epochs=1000, verbose=0)

# Plot training loss and accuracy
plt.figure(figsize=(12, 5))

# Loss plot
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Training Loss Over Epochs')
plt.legend()

# Accuracy plot
plt.subplot(1, 2, 2)
plt.plot(history.history['binary_accuracy'], label='Binary Accuracy', color='green')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Training Accuracy Over Epochs')
plt.legend()

plt.show()


----------------------------------------------------------------------------------------------------------------------------------

andorxor

1. What is a 3-input AND Gate?
A 3-input AND gate outputs 1 only when all three inputs are 1; otherwise, it outputs 0.


A	B	C	A AND B AND C
0	0	0	0
0	0	1	0
0	1	0	0
0	1	1	0
1	0	0	0
1	0	1	0
1	1	0	0
1	1	1	1
ğŸ”¹ 2. Why Use a Neural Network?
You could easily use logic gates directly, but this is an exercise in training a neural network to learn a logical function â€” a foundational task in understanding:

Perceptrons

Forward propagation

Backward propagation

Sigmoid activation

Gradient descent

ğŸ”¹ 3. Network Architecture
For the 3-input AND gate:

Input Layer: 3 nodes (for the 3 inputs)

Hidden Layer: 2â€“4 neurons (experimentation shows 2â€“4 is sufficient)

Output Layer: 1 neuron (sigmoid activated to give a probability-like output)

ğŸ”¹ 4. Activation Function
Sigmoid function is used here:

ğœ
(
ğ‘¥
)
=
1
1
+
ğ‘’
âˆ’
ğ‘¥
Ïƒ(x)= 
1+e 
âˆ’x
 
1
â€‹
 
It outputs values between 0 and 1

Good for binary classification (like AND gates)

ğŸ”¹ 5. Forward Propagation Steps
Let X be the inputs: shape (3, 8)

Hidden Layer Computation:

ğ‘
1
=
ğ‘Š
1
â‹…
ğ‘‹
+
ğ‘
1
Z 
1
â€‹
 =W 
1
â€‹
 â‹…X+b 
1
â€‹
 
ğ´
1
=
sigmoid
(
ğ‘
1
)
A 
1
â€‹
 =sigmoid(Z 
1
â€‹
 )
Output Layer Computation:

ğ‘
2
=
ğ‘Š
2
â‹…
ğ´
1
+
ğ‘
2
Z 
2
â€‹
 =W 
2
â€‹
 â‹…A 
1
â€‹
 +b 
2
â€‹
 
ğ´
2
=
sigmoid
(
ğ‘
2
)
A 
2
â€‹
 =sigmoid(Z 
2
â€‹
 )
A_2 is the final prediction output.

ğŸ”¹ 6. Loss Function
Binary Cross-Entropy Loss is used:

ğ¿
=
âˆ’
1
ğ‘š
âˆ‘
[
ğ‘Œ
â‹…
log
â¡
(
ğ´
)
+
(
1
âˆ’
ğ‘Œ
)
â‹…
log
â¡
(
1
âˆ’
ğ´
)
]
L=âˆ’ 
m
1
â€‹
 âˆ‘[Yâ‹…log(A)+(1âˆ’Y)â‹…log(1âˆ’A)]
Measures how far the prediction is from the ground truth

Smaller is better

ğŸ”¹ 7. Backward Propagation
You calculate gradients for:

W2, b2, W1, b1

Use chain rule to backpropagate the loss

Update parameters using gradient descent

ğŸ”¹ 8. Training Loop
Initialize weights and biases randomly

For n epochs:

Run forward propagation

Compute loss

Run backward propagation

Update weights

ğŸ”¹ 9. Prediction
After training:

Run forward propagation

Classify output as 1 if A2 > 0.5, else 0

ğŸ”¹ 10. Evaluation Metrics
Prediction Accuracy:

Accuracy
=
correctÂ predictions
totalÂ predictions
Ã—
100
%
Accuracy= 
totalÂ predictions
correctÂ predictions
â€‹
 Ã—100%
Loss Curve: plot loss vs. epochs to visualize learning progress

ğŸ”¹ 11. Your Code Summary
You've implemented:

Data setup

Forward propagation

Backward propagation

Training loop with 10,000 epochs

Loss plot

Prediction

Your final model predicts all outputs correctly for the 3-inputÂ ANDÂ gate.


------------------------------------------------------------------------------------------

ann

1. What is a Neural Network?
A Neural Network is a type of machine learning model inspired by the human brain. It's composed of interconnected layers of artificial neurons that learn to recognize patterns by adjusting weights and biases through training.

Neural Networks are the foundation of deep learning, and they power applications like image recognition, speech synthesis, natural language understanding, recommendation systems, and more.

ğŸ”· 2. Structure of a Neural Network
2.1 Neurons
Each neuron receives one or more inputs, multiplies them by weights, adds a bias, and passes the result through an activation function:

ğ‘§
=
âˆ‘
(
ğ‘¤
ğ‘–
â‹…
ğ‘¥
ğ‘–
)
+
ğ‘
ğ‘
=
ğœ™
(
ğ‘§
)
z=âˆ‘(w 
i
â€‹
 â‹…x 
i
â€‹
 )+b
a=Ï•(z)
2.2 Layers
There are typically three types of layers:

Input Layer: Receives the raw data.

Hidden Layers: Do computations and extract features (can be multiple).

Output Layer: Produces the final prediction.

2.3 Parameters
Weights: Determine the strength of connections between neurons.

Biases: Allow shifting of the activation function.

Activation Functions: Introduce non-linearity.

ğŸ”· 3. Why Non-Linearity is Important
Without activation functions, the entire network becomes a linear model no matter how many layers it has. Non-linearity (e.g., ReLU, sigmoid) allows the network to learn complex functions and decision boundaries.

ğŸ”· 4. Activation Functions
These decide whether a neuron should â€œfireâ€ or not by transforming the input.


Function	Output Range	Use Case
ReLU	[0, âˆ)	Default in hidden layers
Sigmoid	(0, 1)	Binary classification
Tanh	(-1, 1)	Better than sigmoid, zero-centered
Softmax	(0, 1), sum = 1	Multi-class output layer
ğŸ”· 5. Forward Propagation
The process of passing inputs through the network to get a prediction:

ğ‘
(
1
)
=
ğœ™
(
ğ‘Š
(
1
)
ğ‘¥
+
ğ‘
(
1
)
)
ğ‘
(
2
)
=
ğœ™
(
ğ‘Š
(
2
)
ğ‘
(
1
)
+
ğ‘
(
2
)
)
â€¦
ğ‘¦
^
=
OutputÂ LayerÂ Activation
a 
(1)
 =Ï•(W 
(1)
 x+b 
(1)
 )
a 
(2)
 =Ï•(W 
(2)
 a 
(1)
 +b 
(2)
 )
â€¦
y
^
â€‹
 =OutputÂ LayerÂ Activation
ğŸ”· 6. Loss Function
Measures how far the prediction is from the actual value. Common loss functions:


Type	Loss	Use Case
Binary Crossentropy	For binary classification	
Categorical Crossentropy	Multi-class classification (with one-hot targets)	
Sparse Categorical Crossentropy	Multi-class (with integer targets)	
MSE (Mean Squared Error)	Regression	
MAE (Mean Absolute Error)	Regression with outliers	
ğŸ”· 7. Backpropagation and Gradient Descent
Backpropagation:
Computes the gradients of the loss with respect to each parameter using the chain rule.

Propagates errors backward through the network.

Gradient Descent:
Updates weights and biases to minimize loss:

ğœƒ
:
=
ğœƒ
âˆ’
ğœ‚
â‹…
âˆ‚
ğ¿
âˆ‚
ğœƒ
Î¸:=Î¸âˆ’Î·â‹… 
âˆ‚Î¸
âˆ‚L
â€‹
 
Where:

ğœƒ
Î¸: weights or biases

ğœ‚
Î·: learning rate

ğ¿
L: loss function

ğŸ”· 8. Optimizers
Algorithms used to update weights efficiently:


Optimizer	Characteristics
SGD	Basic, may converge slowly
Momentum	Adds past gradients to current
RMSprop	Adapts learning rate per parameter
Adam	Combines momentum + RMSprop (most used)
ğŸ”· 9. Epochs, Batches, and Iterations
Epoch: One full pass over the training dataset

Batch size: Number of samples processed before weights are updated

Iteration: One update step (i.e., one batch)

ğŸ”· 10. Underfitting vs Overfitting
Underfitting: Model is too simple (high bias)

Overfitting: Model is too complex (high variance)

Solutions:
Regularization (L1, L2)

Dropout

Early stopping

More data / data augmentation

ğŸ”· 11. Evaluation Metrics
Accuracy: Overall correctness

Precision / Recall / F1 Score: For imbalanced datasets

Confusion Matrix: Breakdown of TP, FP, TN, FN

AUC-ROC: For binaryÂ classifiers

----------------------------------------------------------------------------------------------------

cnn

1. What is a CNN?
A Convolutional Neural Network (CNN) is a specialized type of neural network designed to process grid-like data, such as images (2D pixel arrays) or even time series.

Unlike traditional feedforward neural networks, CNNs use convolutional layers to automatically extract hierarchical features from the input â€” like edges, textures, shapes, and objects.

ğŸ”· 2. Why CNNs Over Feedforward Networks?
Fewer parameters: CNNs reuse filters (weights), which drastically reduces the number of learnable parameters compared to dense networks.

Translation invariance: CNNs detect features irrespective of their position in the image.

Local connectivity: They look at local regions, not the entire image at once â€” mimicking the human visual cortex.

ğŸ”· 3. Architecture of a CNN
ğŸ”¹ a. Input Layer
Typically an image in shape (height Ã— width Ã— channels).
For example: a 64Ã—64 RGB image â†’ shape: (64, 64, 3)

ğŸ”¹ b. Convolutional Layer
Applies a filter/kernel (e.g., 3x3 or 5x5) that slides across the image and extracts features.

Each filter detects a specific feature (e.g., edge, color blob).

The filter performs an element-wise multiplication followed by a sum:

FeatureÂ map
=
Input
âˆ—
Kernel
+
Bias
FeatureÂ map=Inputâˆ—Kernel+Bias
Output: Feature Map (also called an activation map or convolved feature)

ğŸ”¹ c. Activation Function (ReLU)
Adds non-linearity to the output of each convolution.

Commonly used: ReLU (Rectified Linear Unit) â†’ f(x) = max(0, x)

It speeds up training and prevents vanishing gradients.

ğŸ”¹ d. Pooling Layer
Downsamples the feature maps to reduce dimensions and computations.

Types:

Max Pooling: Takes the maximum value from a patch.

Average Pooling: Takes the average of values in the patch.

Example: A 2x2 max pool reduces a 4x4 feature map to 2x2.

ğŸ”¹ e. Flatten Layer
Converts the 2D feature maps into a 1D vector before feeding into fully connected layers.

ğŸ”¹ f. Fully Connected (Dense) Layer
Traditional neural network layers that learn non-linear combinations of features for classification or regression.

ğŸ”¹ g. Output Layer
Uses softmax for multi-class classification.

Uses sigmoid for binary classification.

ğŸ”· 4. CNN Parameters and Hyperparameters

Component	Description
Filter size	Usually 3x3 or 5x5
Stride	Steps the filter moves each time (default = 1)
Padding	Maintains image size ('same') or reduces it ('valid')
Depth	Number of filters per conv layer
Pooling size	Usually 2x2
Dropout	Reduces overfitting by randomly turning off neurons
Epochs / Batch size	Controls training iterations
ğŸ”· 5. Feature Extraction Hierarchy
CNNs learn a hierarchical representation of input:

1st layers â†’ basic edges & textures

Middle layers â†’ parts of objects

Last layers â†’ whole object understanding

ğŸ”· 6. Use Cases of CNNs

Use Case	Description
Image Classification	Cats vs Dogs, MNIST
Object Detection	Bounding boxes (YOLO, Faster R-CNN)
Semantic Segmentation	Pixel-wise classification (U-Net, DeepLab)
Facial Recognition	FaceNet, DeepFace
Medical Imaging	Cancer detection, X-ray analysis
Video Analysis	Frame-wise recognition
ğŸ”· 7. Key Advantages of CNNs
Automatically learn spatial hierarchies

Parameter efficient (compared to fully connected layers)

Perform exceptionally well on image, video, and spatial data

ğŸ”· 8. Common Architectures (You Should Know)

Model	Highlights
LeNet-5	First CNN (for digit recognition)
AlexNet	Introduced ReLU & Dropout
VGGNet	Very deep, 3x3 filters
GoogLeNet (Inception)	Multi-scale filtering
ResNet	Skip connections, solves vanishing gradients
MobileNet	Lightweight, for mobile devices
EfficientNet	Scales depth, width, resolution efficiently
ğŸ”· 9. CNN vs Fully Connected Neural Network

Feature	CNN	Fully Connected NN
Feature Learning	Automatic	Manual
Number of Parameters	Fewer	Much more
Best for	Images, video, spatial data	Tabular data
Local Receptive Fields	Yes	No
Translation Invariance	Yes	No
ğŸ”· 10. Summary Workflow
Input Image

â†’ Convolutional Layers (extract features)

â†’ Activation (ReLU)

â†’ Pooling (reduce size)

â†’ Repeat Steps 2â€“4

â†’ Flatten

â†’ Dense Layers

â†’ OutputÂ Prediction

----------------------------------------------------------------------------------------------------------

rnn

1. What is an RNN?
A Recurrent Neural Network (RNN) is a type of neural network specially designed to process sequential data. Unlike feedforward networks, RNNs maintain a memory of previous inputs using a loop mechanism in their architecture.

ğŸ”· 2. Why RNNs?
Traditional neural networks assume inputs are independent of each other. RNNs solve this by:

Capturing temporal dynamics in sequences.

Sharing weights across time steps.

Maintaining context (memory) through hidden states.

ğŸ”· 3. RNN Architecture
ğŸ”¹ a. Input Sequence
A sequence of data:

ğ‘¥
1
,
ğ‘¥
2
,
.
.
.
,
ğ‘¥
ğ‘¡
x 
1
â€‹
 ,x 
2
â€‹
 ,...,x 
t
â€‹
 
where each 
ğ‘¥
ğ‘¡
x 
t
â€‹
  is an element at time step t.

ğŸ”¹ b. Hidden State
At each time step t, RNN maintains a hidden state 
â„
ğ‘¡
h 
t
â€‹
  which captures the information from past inputs.

â„
ğ‘¡
=
tanh
â¡
(
ğ‘Š
ğ‘¥
â„
â‹…
ğ‘¥
ğ‘¡
+
ğ‘Š
â„
â„
â‹…
â„
ğ‘¡
âˆ’
1
+
ğ‘
â„
)
h 
t
â€‹
 =tanh(W 
xh
â€‹
 â‹…x 
t
â€‹
 +W 
hh
â€‹
 â‹…h 
tâˆ’1
â€‹
 +b 
h
â€‹
 )
ğ‘Š
ğ‘¥
â„
W 
xh
â€‹
 : Input weight matrix

ğ‘Š
â„
â„
W 
hh
â€‹
 : Recurrent (loop) weight matrix

â„
ğ‘¡
âˆ’
1
h 
tâˆ’1
â€‹
 : Previous hidden state

tanh
â¡
tanh: Activation function

ğŸ”¹ c. Output
ğ‘¦
ğ‘¡
=
ğ‘Š
â„
ğ‘¦
â‹…
â„
ğ‘¡
+
ğ‘
ğ‘¦
y 
t
â€‹
 =W 
hy
â€‹
 â‹…h 
t
â€‹
 +b 
y
â€‹
 
The output can be:

At each time step (e.g., in time series forecasting)

Only at the final step (e.g., for sentiment classification)

ğŸ”· 4. Types of RNN Architectures

Type	Description
One-to-One	Standard ANN (e.g., image classification)
One-to-Many	Image captioning
Many-to-One	Sentiment analysis
Many-to-Many	Language translation, time series prediction
ğŸ”· 5. RNN Limitations
Vanishing & Exploding Gradients: Long-term dependencies are hard to learn.

Short-Term Memory: Struggles with long sequences.

Training Instability: Due to recurrent weight updates over many time steps.

ğŸ”· 6. Solutions to RNN Limitations
âœ… LSTM (Long Short-Term Memory)
Adds gates to control information flow.

Preserves long-range dependencies better.

âœ… GRU (Gated Recurrent Unit)
Simpler than LSTM with similar performance.

Combines the forget and input gates.

ğŸ”· 7. LSTM Architecture
Each LSTM cell contains:

Forget gate: Decides what information to discard

Input gate: Decides what new information to store

Output gate: Determines the output based on the current cell state

These gates use sigmoid activations to control information flow.

ğŸ”· 8. Activation Functions in RNNs

Function	Use
tanh	For hidden state updates
sigmoid	For gates in LSTM/GRU
softmax	For output probabilities
ğŸ”· 9. Use Cases of RNNs

Domain	Example
NLP	Text generation, translation, sentiment analysis
Time Series	Stock price prediction, weather forecasting
Speech	Recognition, synthesis
Music	Composition, transcription
Healthcare	Patient monitoring, ECG analysis
ğŸ”· 10. Comparison Table

Model	Memory	Complexity	Use Case
Vanilla RNN	Short-term	Low	Simple sequences
LSTM	Long-term	High	Complex dependencies
GRU	Long-term	Medium	Balanced performance
ğŸ”· 11. Bidirectional RNNs
Process input from both directions (forward and backward).

Useful for tasks where context from future matters (e.g., POS tagging).

ğŸ”· 12. Sequence-to-Sequence (Seq2Seq)
Uses two RNNs: one as an encoder, one as a decoder.

Essential for translation, summarization, chatbots.

ğŸ”· 13. Attention Mechanism (Advanced)
Solves bottleneck of Seq2Seq models.

Allows decoder to focus on relevant parts of input at each step.

ğŸ”· 14. Summary Workflow
Input sequence â†’

Embedded or encoded input â†’

RNN / LSTM / GRU layer â†’

(Optional) Attention â†’

Fully connected layers â†’

Output (classification, generation,Â etc.)


----------------------------------------------------------------------------------------------

lstm

1. What is an LSTM?
LSTM is a special type of RNN designed to overcome vanishing gradient and short memory issues. It introduces memory cells and gating mechanisms that allow the network to remember or forget information over long sequences.

Developed by Hochreiter & Schmidhuber in 1997, LSTMs are highly effective for tasks where long-term context is crucial.

ğŸ”· 2. Why Use LSTM Over RNN?

Limitation in RNN	LSTM Advantage
Canâ€™t retain long-term memory	Uses memory cells
Vanishing gradient problem	Uses gated flow of gradients
Hard to decide what to forget or remember	Explicit forget and input gates
ğŸ”· 3. LSTM Cell Architecture
An LSTM unit consists of:

Cell state (
ğ¶
ğ‘¡
C 
t
â€‹
 ) â€” like a conveyor belt that carries memory.

Hidden state (
â„
ğ‘¡
h 
t
â€‹
 ) â€” output at time 
ğ‘¡
t.

Input gate â€” controls what new information to add.

Forget gate â€” controls what information to discard.

Output gate â€” controls what to output.

ğŸ”¹ Gates in LSTM
Letâ€™s denote:

ğ‘¥
ğ‘¡
x 
t
â€‹
  = input at time step t

â„
ğ‘¡
âˆ’
1
h 
tâˆ’1
â€‹
  = previous hidden state

ğ¶
ğ‘¡
âˆ’
1
C 
tâˆ’1
â€‹
  = previous cell state

ğœ
Ïƒ = sigmoid activation

tanh
â¡
tanh = tanh activation

a. Forget Gate
ğ‘“
ğ‘¡
=
ğœ
(
ğ‘Š
ğ‘“
â‹…
[
â„
ğ‘¡
âˆ’
1
,
ğ‘¥
ğ‘¡
]
+
ğ‘
ğ‘“
)
f 
t
â€‹
 =Ïƒ(W 
f
â€‹
 â‹…[h 
tâˆ’1
â€‹
 ,x 
t
â€‹
 ]+b 
f
â€‹
 )
Decides what part of 
ğ¶
ğ‘¡
âˆ’
1
C 
tâˆ’1
â€‹
  to forget.

Values close to 0 mean "forget", values close to 1 mean "keep".

b. Input Gate
ğ‘–
ğ‘¡
=
ğœ
(
ğ‘Š
ğ‘–
â‹…
[
â„
ğ‘¡
âˆ’
1
,
ğ‘¥
ğ‘¡
]
+
ğ‘
ğ‘–
)
i 
t
â€‹
 =Ïƒ(W 
i
â€‹
 â‹…[h 
tâˆ’1
â€‹
 ,x 
t
â€‹
 ]+b 
i
â€‹
 )
ğ¶
~
ğ‘¡
=
tanh
â¡
(
ğ‘Š
ğ¶
â‹…
[
â„
ğ‘¡
âˆ’
1
,
ğ‘¥
ğ‘¡
]
+
ğ‘
ğ¶
)
C
~
  
t
â€‹
 =tanh(W 
C
â€‹
 â‹…[h 
tâˆ’1
â€‹
 ,x 
t
â€‹
 ]+b 
C
â€‹
 )
ğ‘–
ğ‘¡
i 
t
â€‹
  decides what portion of new candidate memory 
ğ¶
~
ğ‘¡
C
~
  
t
â€‹
  to store.

c. Update Cell State
ğ¶
ğ‘¡
=
ğ‘“
ğ‘¡
â‹…
ğ¶
ğ‘¡
âˆ’
1
+
ğ‘–
ğ‘¡
â‹…
ğ¶
~
ğ‘¡
C 
t
â€‹
 =f 
t
â€‹
 â‹…C 
tâˆ’1
â€‹
 +i 
t
â€‹
 â‹… 
C
~
  
t
â€‹
 
Combines whatâ€™s retained from the past and whatâ€™s newly added.

d. Output Gate
ğ‘œ
ğ‘¡
=
ğœ
(
ğ‘Š
ğ‘œ
â‹…
[
â„
ğ‘¡
âˆ’
1
,
ğ‘¥
ğ‘¡
]
+
ğ‘
ğ‘œ
)
o 
t
â€‹
 =Ïƒ(W 
o
â€‹
 â‹…[h 
tâˆ’1
â€‹
 ,x 
t
â€‹
 ]+b 
o
â€‹
 )
â„
ğ‘¡
=
ğ‘œ
ğ‘¡
â‹…
tanh
â¡
(
ğ¶
ğ‘¡
)
h 
t
â€‹
 =o 
t
â€‹
 â‹…tanh(C 
t
â€‹
 )
Decides what information to output from the cell.

ğŸ”· 4. Summary of LSTM Flow
At each time step:

Forget irrelevant past info.

Update cell state with new info.

Output a filtered view of the current state.

ğŸ”· 5. Key Features of LSTM

Feature	Benefit
Memory cell	Stores info over long time spans
Gating mechanisms	Learn what to forget, remember, and output
Better gradient flow	Avoids vanishing/exploding gradients
Shared weights	Reduces number of parameters
Supports variable-length input	Ideal for time-series, language, etc.
ğŸ”· 6. Variants of LSTM

Variant	Description
Peephole LSTM	Gates also look at the cell state
Bidirectional LSTM (BiLSTM)	Reads sequence both forward and backward
Stacked LSTM	Multiple LSTM layers for more depth
ConvLSTM	Combines LSTM with convolutions for spatial-temporal data
ğŸ”· 7. Activation Functions Used

Gate/Component	Activation
Forget/Input/Output gates	Sigmoid (
ğœ
Ïƒ)
Cell input/output	tanh (
tanh
â¡
tanh)
ğŸ”· 8. Applications of LSTM

Domain	Example
Natural Language Processing	Language modeling, translation, text generation
Speech Processing	Speech recognition, synthesis
Finance	Stock price prediction
Healthcare	Patient health trend analysis
Video Analysis	Gesture recognition, captioning
ğŸ”· 9. Comparison with Other Models

Model	Pros	Cons
RNN	Simple	Forgets long-term context
LSTM	Captures long-term dependencies	Slower, more parameters
GRU	Faster than LSTM, similar performance	Slightly less expressive
ğŸ”· 10. LSTM in Practice (Conceptual Flow)
css
Copy
Edit
for time step t:
    forget = sigmoid(...)
    input = sigmoid(...)
    candidate = tanh(...)
    cell_state = forget * old_cell + input * candidate
    output_gate = sigmoid(...)
    hidden_state = output_gate * tanh(cell_state)
ğŸ§  Final Summary
LSTM is an enhanced version of RNN that solves its memory limitations through gates and a dedicated memory cell. It is robust for long-sequence data, crucial in domains like language, audio, and time series.

Understanding LSTM is a stepping stone to more advanced models like GRU, Seq2Seq, and evenÂ Transformers.

-----------------------------------------------------------------------------------------------------------

img preproc


What is Image Preprocessing?
Image preprocessing is the set of operations performed on raw image data to:

Enhance quality

Normalize features

Reduce noise

Ensure consistency in input

Make learning easier for neural networks

ğŸ”· 2. Why is Image Preprocessing Important?

Benefit	Description
Standardization	Ensures all images are of same size and format
Improved convergence	Helps models learn faster and more accurately
Noise reduction	Removes irrelevant variations
Feature enhancement	Improves contrast, edges, etc.
Reduced overfitting	Augmentation increases diversity of training data
ğŸ”· 3. Common Image Preprocessing Techniques
âœ… a. Resizing
Rescales image dimensions (e.g., to 224x224 for ResNet)

Required since most models expect a fixed input size

python
Copy
Edit
from PIL import Image
img = Image.open("img.jpg").resize((224, 224))
âœ… b. Grayscale Conversion
Reduces computational complexity by converting RGB to a single channel

Useful when color is not informative

python
Copy
Edit
gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
âœ… c. Normalization / Standardization

Method	Formula	Use
Min-Max Scaling	
(
ğ‘¥
âˆ’
ğ‘š
ğ‘–
ğ‘›
)
/
(
ğ‘š
ğ‘
ğ‘¥
âˆ’
ğ‘š
ğ‘–
ğ‘›
)
(xâˆ’min)/(maxâˆ’min)	Maps to [0, 1]
Z-score	
(
ğ‘¥
âˆ’
ğœ‡
)
/
ğœ
(xâˆ’Î¼)/Ïƒ	Maps to mean = 0, std = 1
Divide by 255	
ğ‘¥
/
255
x/255	Quick scaling for pixel values (0â€“255)
python
Copy
Edit
img = img / 255.0
âœ… d. Noise Removal (Denoising)
Removes irrelevant patterns using:

Gaussian Blur

Median filter

Bilateral filter

python
Copy
Edit
blurred_img = cv2.GaussianBlur(img, (5,5), 0)
âœ… e. Thresholding / Binarization
Converts image to binary (black & white)

Useful for object detection and OCR

python
Copy
Edit
_, thresh = cv2.threshold(gray_img, 127, 255, cv2.THRESH_BINARY)
âœ… f. Histogram Equalization
Enhances contrast of grayscale images

Useful in medical imaging or low-light conditions

python
Copy
Edit
equalized = cv2.equalizeHist(gray_img)
âœ… g. Edge Detection
Detects boundaries of objects using:

Sobel filter

Canny edge detector

python
Copy
Edit
edges = cv2.Canny(gray_img, 100, 200)
âœ… h. Data Augmentation
Artificially enlarges dataset by applying:

Rotation

Zoom

Horizontal/Vertical Flip

Brightness Adjustment

Shearing

Used to make the model more robust and prevent overfitting.

python
Copy
Edit
from keras.preprocessing.image import ImageDataGenerator

datagen = ImageDataGenerator(rotation_range=20, zoom_range=0.2, horizontal_flip=True)
ğŸ”· 4. Image Preprocessing Pipelines in Frameworks

Framework	Functionality
Keras	ImageDataGenerator, tf.image
PyTorch	torchvision.transforms.Compose()
OpenCV	Direct access to image manipulations
PIL	Basic resizing, cropping, etc.
ğŸ”· 5. Best Practices
Always resize images to model-expected input size

Normalize pixel values

Use data augmentation on training set only

Keep preprocessing consistent between training and inference

Use grayscale when color isnâ€™t relevant to reduce computation

ğŸ”· 6. When to Preprocess?

Stage	Techniques
Before Training	Resizing, normalization, augmentation
Before Prediction	Same resizing and normalization as training
After Prediction	Optional denormalization for visualization
ğŸ”· 7. Advanced Techniques (Optional)

Technique	Use Case
CLAHE (Contrast Limited Adaptive Histogram Equalization)	Local contrast enhancement
Fourier Transform	Frequency domain filtering
Color Space Conversion (HSV, LAB)	Better illumination handling
Padding and Cropping	Spatial transformations
ğŸ§  Final Summary
Image preprocessing ensures that images are clean, consistent, and correctly scaled so that deep learning models can learn patterns effectively. Itâ€™s essential for better accuracy, faster training, and robust performance in computerÂ visionÂ tasks.


---------------------------------------------------------------------------------------------------

act functn

What Are Activation Functions?
Activation functions determine whether a neuron should be activated (fire) or not by applying a non-linear transformation to its input. They are crucial because:

Without them, neural networks would only be able to learn linear relationships, no matter how many layers they have.

They allow deep neural networks to learn complex patterns (e.g., image features, language structures, etc.).

ğŸ“Œ Why Non-Linearity?
If you stack multiple linear functions, the result is still linear. Non-linear activation functions let the network approximate any arbitrary function â€” enabling it to learn non-linear decision boundaries, crucial for tasks like image recognition, NLP, etc.

âš™ Types of Activation Functions
1. Sigmoid (Logistic) Function
Formula:

ğœ
(
ğ‘¥
)
=
1
1
+
ğ‘’
âˆ’
ğ‘¥
Ïƒ(x)= 
1+e 
âˆ’x
 
1
â€‹
 
Output range: (0, 1)

Use Case: Binary classification (usually in the final output layer)

âœ… Pros:
Probabilistic interpretation

Smooth gradient

âŒ Cons:
Vanishing gradients

Output not zero-centered

Slow convergence

2. Tanh (Hyperbolic Tangent)
Formula:

tanh
â¡
(
ğ‘¥
)
=
ğ‘’
ğ‘¥
âˆ’
ğ‘’
âˆ’
ğ‘¥
ğ‘’
ğ‘¥
+
ğ‘’
âˆ’
ğ‘¥
tanh(x)= 
e 
x
 +e 
âˆ’x
 
e 
x
 âˆ’e 
âˆ’x
 
â€‹
 
Output range: (-1, 1)

Use Case: Hidden layers (better than sigmoid)

âœ… Pros:
Zero-centered output

Stronger gradients than sigmoid

âŒ Cons:
Vanishing gradient problem still exists

3. ReLU (Rectified Linear Unit)
Formula:

ğ‘“
(
ğ‘¥
)
=
max
â¡
(
0
,
ğ‘¥
)
f(x)=max(0,x)
Output range: [0, âˆ)

Use Case: Default choice for hidden layers

âœ… Pros:
Simple & fast

Sparse activation (some neurons output 0)

Reduces vanishing gradient issue

âŒ Cons:
"Dead neurons" problem (neuron gets stuck outputting 0)

Not zero-centered

4. Leaky ReLU
Formula:

ğ‘“
(
ğ‘¥
)
=
{
ğ‘¥
ifÂ 
ğ‘¥
â‰¥
0
ğ›¼
ğ‘¥
ifÂ 
ğ‘¥
<
0
f(x)={ 
x
Î±x
â€‹
  
ifÂ xâ‰¥0
ifÂ x<0
â€‹
 
where 
ğ›¼
Î± is a small positive constant (e.g., 0.01)

âœ… Pros:
Fixes the "dead neuron" issue in ReLU

âŒ Cons:
Slightly more complex

The value of Î± is a hyperparameter

5. Parametric ReLU (PReLU)
Like Leaky ReLU, but Î± is learned during training.

6. ELU (Exponential Linear Unit)
Formula:

ğ‘“
(
ğ‘¥
)
=
{
ğ‘¥
ifÂ 
ğ‘¥
>
0
ğ›¼
(
ğ‘’
ğ‘¥
âˆ’
1
)
ifÂ 
ğ‘¥
â‰¤
0
f(x)={ 
x
Î±(e 
x
 âˆ’1)
â€‹
  
ifÂ x>0
ifÂ xâ‰¤0
â€‹
 
âœ… Pros:
Produces negative outputs, making it zero-centered

Reduces vanishing gradient

âŒ Cons:
Computationally expensive

7. Swish
Formula:

ğ‘“
(
ğ‘¥
)
=
ğ‘¥
â‹…
ğœ
(
ğ‘¥
)
f(x)=xâ‹…Ïƒ(x)
Proposed by Google, performs better in deeper networks.

âœ… Pros:
Smooth, non-monotonic

Improves performance over ReLU in many deep nets

8. Softmax
Formula:

Softmax
(
ğ‘§
ğ‘–
)
=
ğ‘’
ğ‘§
ğ‘–
âˆ‘
ğ‘—
ğ‘’
ğ‘§
ğ‘—
Softmax(z 
i
â€‹
 )= 
âˆ‘ 
j
â€‹
 e 
z 
j
â€‹
 
 
e 
z 
i
â€‹
 
 
â€‹
 
Use Case: Multi-class classification (output layer)

âœ… Pros:
Outputs sum to 1 â†’ Probabilities

âŒ Cons:
Sensitive to outliers

ğŸ§  Summary Table

Function	Range	Use Case	Pros	Cons
Sigmoid	(0, 1)	Binary classification	Probabilistic output	Vanishing gradients
Tanh	(-1, 1)	Hidden layers	Zero-centered	Still vanishing gradients
ReLU	[0, âˆ)	Hidden layers	Fast, sparse	Dead neurons
Leaky ReLU	~(-âˆ, âˆ)	Hidden layers	Fixes ReLU issue	Not widely used by default
PReLU	~(-âˆ, âˆ)	Hidden layers	Learns best Î±	More complex
ELU	~(-1, âˆ)	Hidden layers	Zero-centered	Slower to compute
Swish	~(-0.3, âˆ)	Deep networks	Smooth, flexible	New, less tested
Softmax	[0, 1]	Output for multiclass	Probabilities	SensitiveÂ toÂ outliers

---------------------------------------------------------------------------------------------------
